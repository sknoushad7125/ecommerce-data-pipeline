# ğŸš€ E-Commerce Analytics Data Pipeline

An end-to-end **production-style Data Engineering pipeline** built using **Python, PySpark, PostgreSQL, and Apache Airflow**.  
This project simulates how real-world data teams ingest, transform, store, and automate analytics workflows.

---

## ğŸ“Œ Project Overview

Modern companies rely on automated data pipelines to generate business insights.  
This project demonstrates a **complete batch ETL pipeline** for e-commerce order data, from raw ingestion to analytics-ready storage with orchestration and monitoring.

---

## ğŸ—ï¸ Architecture

Raw Data (CSV / API)
â†“
Python Ingestion
â†“
PySpark ETL Transformations
â†“
PostgreSQL Data Warehouse
â†“
Apache Airflow (Scheduling & Monitoring)


---

## ğŸ”§ Tech Stack

- **Python** â€“ ingestion, loading, utilities  
- **PySpark** â€“ scalable ETL and transformations  
- **PostgreSQL** â€“ analytics data warehouse  
- **Apache Airflow** â€“ workflow orchestration  
- **SQL** â€“ data modeling & validation  

---

## ğŸ“‚ Project Structure

```text
ecommerce_data_pipeline/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Raw input data
â”‚ â””â”€â”€ processed/ # Spark output
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ ingestion/ # Python ingestion scripts
â”‚ â”œâ”€â”€ transform/ # PySpark ETL jobs
â”‚ â””â”€â”€ load/ # PostgreSQL load scripts
â”‚
â”œâ”€â”€ airflow/
â”‚ â”œâ”€â”€ dags/ # Airflow DAGs
â”‚ â”œâ”€â”€ logs/
â”‚ â””â”€â”€ plugins/
â”‚
â”œâ”€â”€ config/
â”œâ”€â”€ logs/
â””â”€â”€ README.md
```

---

## ğŸ—ƒï¸ Data Model

### Fact Table
**fact_orders**
- order_id  
- order_date  
- customer_id  
- product_id  
- quantity  
- price  
- revenue  

Designed for **analytics & reporting use cases**.

---

## âš™ï¸ Pipeline Workflow

1. **Ingestion**
   - Raw order data loaded using Python

2. **Transformation**
   - PySpark cleans data and calculates derived metrics (revenue)

3. **Loading**
   - Transformed data stored in PostgreSQL warehouse

4. **Orchestration**
   - Airflow DAG schedules and monitors the pipeline
   - Task dependencies, retries, and logging enabled

5. **Data Quality**
   - Post-load row count validation

---

## ğŸ§ª Example Airflow DAG Tasks

- spark_transform_orders  
- load_to_postgres  
- check_row_count  

---


## â–¶ï¸ How to Run Locally

### 1. Clone Repository
```bash
git clone https://github.com/sknoushad7125/ecommerce-data-pipeline.git
cd ecommerce-data-pipeline
2. Create Virtual Environment
python3.10 -m venv venv
source venv/bin/activate
3. Install Dependencies
pip install pyspark psycopg2-binary apache-airflow
4. Run Spark Job
spark-submit scripts/transform/spark_transform_orders.py
5. Load to PostgreSQL
python scripts/load/load_to_postgres.py
6. Start Airflow
airflow webserver
airflow scheduler
Access UI at:
http://localhost:8080
```


Key Learnings:

Building scalable ETL pipelines using Spark
Orchestrating workflows with Apache Airflow
Debugging Spark, Java, and Airflow environment issues
Designing analytics-ready warehouse tables
Applying real-world data engineering best practices

Future Improvements:

Incremental data loading
Cloud migration (AWS S3, EMR, RDS)
Kafka-based streaming ingestion
dbt-based transformations
Dockerized deployment

Author

Noushad Sk

Aspiring AI-Powered Data Engineer

---
